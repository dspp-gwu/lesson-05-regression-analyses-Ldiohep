{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis - A Plan for This Lesson\n",
    "\n",
    "We're going to learn a little bit about traditional statistical analyses.  The crucial piece here is not that you to fully understand all of traditional statistics -- that's another class -- but rather to gain a sense of what it can (and more importantly, what it *can not*) do.  \n",
    "\n",
    "Traditional statistical analyses are how many people do exploratory research.  But do not make the classic research mistake of thinking that patterns that you find in your data represent reality. Regression analyses are powerful, but inept use of regressions also lays the groundwork for much of the replication crisis currently engulfing the social sciences.  If you haven't heard about [data dredging](https://en.wikipedia.org/wiki/Data_dredging), p-hacking and over-fitting, get used to those terms.  They are bane of modern research existence, and they are still really, really widespread.  More than that, even when using the best machine learning practices, people make all kinds of mistakes about what data should and should not be included in their particular application.  We'll talk a bit more about that as well. \n",
    "\n",
    "That plan for this lesson is to get some data sets loaded, make sure they're all properly geocoded and merged, and then look for patterns in the data.  We'll find some -- but it might not be as simple as it appears at first. \n",
    "\n",
    "\n",
    "# Startup - Installing and Importing\n",
    "\n",
    "First things first: let's get all our packages installed and imported.  You'll see that we need to install geopandas and xtree. thankfully, statsmodels (the package we'll use to run our regressions) and seaboarn (the package we'll use to make some pretty plots) are both already installed, and we can just import them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install geopandas -yq\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf \n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib  inline\n",
    "import seaborn as sns; sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - reading in data\n",
    "\n",
    "Let's read in some District crime data. Eventually we'll deal with crime & time in a more complex manner. \n",
    "For now, though, let's just have a look crime from a single year: 2010. Find the data on Open Data DC. Instead of downloading the data to your laptop, just copy the url for the \"spreadsheet\" under the download menu. Pandas is smart enough to just import it via the static url from that site. In a single line, read the data into a dataframe called \"crimes\" by using the pd.read_csv() function, and the url you just copied (inside quotation marks, because it's a string). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crimes = pd.read_csv('https://opendata.arcgis.com/datasets/fdacfbdda7654e06a161352247d3a2f0_34.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! One line and we have some great data from the web!  You'll see that we can't always do that, but it's nice when we can. \n",
    "\n",
    "# 02 - exploring the data\n",
    "\n",
    "Ok, let's see what's in this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crimes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the types of offenses recorded here are probably labeled ... OFFENSE!  As we've seen before, \n",
    "one nice way to get a sense of the frequencies in our data is to use .value_counts().  Let's see how many crimes were committed in each ward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crimes.WARD.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a mix of every kind of crime. Let's see if we can focus on violent crimes.  To figure out which of those we have in this data set, print out the value counts for the column named \"OFFENSE\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the most violent offenses are, thankfully, also among the least common. But assoault with a dangerous weapon is still pretty common.  Let's focus on violent crimes against persons.  \n",
    "\n",
    "# Subsetting\n",
    "Although the final statement might seem complex, it's actually fairly straightforward once you get the hang of it.  \n",
    "\n",
    "\n",
    "## 03 - True and false\n",
    "\n",
    "First, let's create a set of logical statments that give us a \"True\" for each of the crimes we want.  \n",
    "\n",
    "```crimes['OFFENSE']=='HOMICIDE'```\n",
    "\n",
    "That is a piece of code that asks pandas \"does the values in the column OFFENSE equal 'HOMICIDE', and it will return True or False for every single row in the dataframe. Try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that helpful?  Because if we wanted to create a new dataframe with just homicides, we can now do the following: \n",
    "\n",
    "``` crimes[crimes['OFFENSE']=='HOMICIDE'] ```\n",
    "\n",
    "Try it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how we nested that line from above inside the square brackets after the crimes dataframe?  That tells pandas to return only those rows where the statement is true. And since it is only true where the OFFENSE is HOMICIDE, it is taking all the homicide rows from the crimes dataframe and copying them into the new homicide dataframe. \n",
    "\n",
    "But... we really want more than homicides.  So, we're going to combine three statments together: \n",
    "\n",
    "```\n",
    "crimes['OFFENSE']=='HOMICIDE'\n",
    "crimes['OFFENSE']=='ASSAULT W/DANGEROUS WEAPON'\n",
    "crimes['OFFENSE']=='SEX ABUSE'\n",
    "\n",
    "```\n",
    "And we'll link them with a funny symbol that means OR in python: |.  It's that straight line just above the downward slash over the enter/return key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = crimes[(crimes['OFFENSE']=='HOMICIDE') | \n",
    "                (crimes['OFFENSE']=='ASSAULT W/DANGEROUS WEAPON') |\n",
    "                (crimes['OFFENSE']=='SEX ABUSE')]\n",
    "print(crimes.OFFENSE.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our violent crimes against persons in DC from 2010. Time to add some geography. \n",
    "\n",
    "# 04 - adding census data and geography\n",
    "\n",
    "To read in our census data, I like to use geojson.  Geojson combines the shape data with other data in a single file. It tends to be large, but it's extremely convenient.  Let's try grabbing it from Opend data DC .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tracts = gpd.read_file('https://opendata.arcgis.com/datasets/6969dd63c5cb4d6aa32f15effb8311f3_8.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What?  I though pandas could read in data stright from from a URL?  Well: pandas can; geopandas can't.  We'll have to download this before loading into geopandas. But let's do this like the pros do.  To grab the data (the URL is available under the \"APIs\" dropdown menu as \"geojson\"), we'll use the fantastic unix command \"wget\".  We can take the crazy name in the URL and given a more reasonable name by using the -O option, thusly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://opendata.arcgis.com/datasets/6969dd63c5cb4d6aa32f15effb8311f3_8.geojson -O census.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - reading the data into geopandas\n",
    "\n",
    "OK, recall that reading in to geopandas is as easy as ```gpd.read_file()```. Load that file named \"census.geojson\" into a geopandas dataframe called \"census\".  It's already in your working direcory in the cloud thanks to wget.  :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census = gpd.read_file('census.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.  Now let's see what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blargh.  Why did the census name everything with code?  I guess because computers used to have tiny, tiny eight character naming limits. Well, I'll tell you what.  FAGI is the \"Federal Adjusted Gross Income\", and it's a very valuable little piece of data to have. There are also data on race, ethnicity and -- if we do a little math -- population density in here.  \n",
    "\n",
    "But before we get into all that, let's take the data for violent crimes in 2010 and plot it inside our census tracts like we learned last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "geometry = [Point(xy) for xy in zip(crimes.LONGITUDE.apply(float), crimes.LATITUDE.apply(float))]\n",
    "crs = {'init': 'epsg:4326'}\n",
    "points = gpd.GeoDataFrame(crimes, crs=crs, geometry=geometry)\n",
    "fig, ax = plt.subplots()\n",
    "census.plot(ax=ax, color='red')\n",
    "points.plot(ax=ax, color='black', marker='.', markersize=5)\n",
    "ax.set_aspect('equal')  # remember, this is what we need to do in order to not have a crazy wide map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to aggregate the crime into each census tract, and to do that we need to join that crime data with the census tract data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_crimes = gpd.sjoin(census, points, how='left', op='intersects')\n",
    "geo_crimes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in pretty much the format we want.  We have a bunch of census tracts wtih crimes aggregated.  But what we still don't know is how many crimes are committed *per capita*, which is far more informative than just how many crimes have occured. For that we need to know the total population in each census tract.  And that is actually in the column named \"P0010001\". What a great name.  The best.  All right, putting it all together, just like we did last week, we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_percap = pd.DataFrame(geo_crimes.OBJECTID_left.value_counts()*100000/geo_crimes.P0010001.sum())\n",
    "crimes_percap.columns = ['crimes_percap']\n",
    "tracts_crimes = census.merge(crimes_percap, how=\"left\", left_on='OBJECTID', right_index=True)\n",
    "tracts_crimes.plot(column='crimes_percap', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - drawing some dots and some lines\n",
    "\n",
    "As this lesson is about the traditional statistical technique of regression, we're going to do what researchers did for ages before they had machine learning algorithms: look at some simple patterns.  If X and Y are positively correlated, that means that the higher X is, at least on average, the higher Y is.  And if X and Y are negatively correlated, when X goes higher, Y goes lower. \n",
    "\n",
    "Let's draw a fairly simple plot, then we can try to see if there is a pattern we can discern.  Try this: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tracts_crimes[['FAGI_MEDIAN_2010', 'crimes_percap']])\n",
    "df.plot.scatter('FAGI_MEDIAN_2010', 'crimes_percap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a pretty steep falloff as income goes up.   \n",
    "\n",
    "If you drew a line through those dots, what would it look like?  We'll try a few statistical technicques, but you could probably just draw a line with you finger on the screen. Do it slant upward or downward as you move from left to right?  \n",
    "\n",
    "Better than imagining a line, we could actually draw using math.  Or, better still, let the comupter draw one using math!  To do this, we'll use the delightful package called \"seaborn\" that we loaded at the top of the page. \n",
    "\n",
    "Seaborn is going to fit a line using simple linear regression, a line that has the least \"residual error\" possible.  This means that if you moved the line in any way, the overall sum of the distances between the dots and the line with INCREASE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"FAGI_MEDIAN_2010\", y=\"crimes_percap\", data=tracts_crimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That line is as close to ALL the dots as a straight line can be.  This is known as linear regression. And we can actually figure out a lot fo things about how good a fit it is if we use the statsmodels package.  \n",
    "\n",
    "Let's fit a model that has just those two variables: income and crime rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'crimes_percap ~ FAGI_MEDIAN_2010'\n",
    "\n",
    "results = smf.ols(model, data=tracts_crimes).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from this output we can learn a lot.  First, the R-squared tells us about what percentage of the variation in the crime rate is also associated with variation in median income.  So there's roughly at 32% association.  That's better than nothing, but pretty far short of 100%.  And, if you are like puzzles, you'll be tempted to see if you can \"explain\" more of the variation.  That's good -- but also potentially bad. \n",
    "\n",
    "Let's see what we can do with a few reasonable adjustments. First, if you were drawing a line through those dots, and you could bend it a bit, would it be a better fit?  Probably, right?  the dots all seem to cluster near the axes.  In fact, if we want to, we can check this intuition by drawing what is called a \"local fit\" regression line. This is often referred to as \"loess\" or \"lowess\".  And seaborn has a very nice one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"FAGI_MEDIAN_2010\", y=\"crimes_percap\", data=tracts_crimes)\n",
    "sns.regplot(x=\"FAGI_MEDIAN_2010\", y=\"crimes_percap\", data=tracts_crimes, lowess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a statistician sees a plot like this, they think: LOG scale.  A log scale just means reducing a number by a factor. So: \n",
    "\n",
    "```\n",
    "number  log\n",
    "0.001   -3\n",
    "0.01    -2\n",
    "0.1     -1\n",
    "1        0\n",
    "10       1\n",
    "100      2\n",
    "1000     3\n",
    "```\n",
    "\n",
    "The highly technical term for this that I like to use is \"unsmooshing\".  Let's try that here by applying a log transformation to each of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracts_crimes['income_log'] = tracts_crimes['FAGI_MEDIAN_2010'].apply(np.log)\n",
    "tracts_crimes['crime_log'] = tracts_crimes['crimes_percap'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the same from above (the two lines for the seaborn plots) and subsitute the variables we just recreated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"income_log\", y=\"crime_log\", data=tracts_crimes)\n",
    "sns.regplot(x=\"income_log\", y=\"crime_log\", data=tracts_crimes, lowess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is actually very little difference between straight line in the linear regression (blue), and the \"local fit\" line in the the lowess regression (green). So it looks like we were correct -- this is a log distribution in both directions.  You'll find that income data are almost always like this: people tend to cluster at the bottom of the scale, and at the top people are really, really rich. This pattern is true of many other measures as well. \n",
    "\n",
    "In human terms, what this means is that the dropoff in the crime rate associated with increased median income in a census tract are both very steep, but grows less steep over time. Increasingly larger amounts of income (log of the income) are correlated with a increasingly smaller reductions in the violent crime rate (log of crime percapita).  Another way of putting it is that the loss of each subsequent dollar of income is associated with a substantially larger increase in violent crime.  Many data sets contain log distributions -- that is, they are both concentrated at the low end of the scale. \n",
    "\n",
    "And, indeed, this does make for a better statistical fit when we have a look at the regression statistics in statsmodels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'np.log(crimes_percap) ~ np.log(FAGI_MEDIAN_2010) '\n",
    "\n",
    "results = smf.ols(model, data=tracts_crimes).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, median income appears to explain away nearly 50% of the variation in violent crime.  Or is it the other way around?  Does poverty lead to violence, or does violence drive away money?  It's not an idle question, and a great deal of thought needs to go into the answer before we even begin to think about using this model to inform a particular public policy or practice. \n",
    "\n",
    "There are other ways of increasing fit, and we could have chosen from many different types of transformations or models of the data.  \n",
    "\n",
    "# Danger, Will Robinson, Danger!\n",
    "\n",
    "But for the reckless, why stop at one variable?  The world is full of data, and researchers have thrown a lot of data crime to see what is correlated and what drives up their R-squared. I'll create a few more variables to add to the mix, and you can play around with the formulas in the equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracts_crimes['percent_white'] = tracts_crimes['P0010003'] / tracts_crimes['P0010001']\n",
    "tracts_crimes['percent_black'] = tracts_crimes['P0010004'] / tracts_crimes['P0010001']\n",
    "tracts_crimes['percent_asian'] = tracts_crimes['P0010006'] / tracts_crimes['P0010001']\n",
    "tracts_crimes['population_density'] = tracts_crimes['P0010001'] / tracts_crimes['SQ_MILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'crime_log ~ income_log + population_density + percent_white + percent_black + percent_asian '\n",
    "results = smf.ols(model, data=tracts_crimes).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crazy interactions\n",
    "\n",
    "Just to show you how crazy these models can get, I'll fully interact all the variables, meaning that I'm assuming all the variables interact with each other in a way that matters to crime.  I have not real theory -- indeed I doubt one could come up with a reasonable one -- but you should know that p-hackers tend to interact almost everthing, then drop the less promsing interactions.  The theory the data supports is then invented after they have found novel pattern in the data. Publication gold! But terrible for science and even worse for public policy. \n",
    "\n",
    "To commit this statistical crime, run the same commands, but inside the formual, replace the plus sign with an asterisk (\\* instead of +)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'crime_log ~ income_log * population_density * percent_white * percent_black * percent_asian '\n",
    "results = smf.ols(model, data=tracts_crimes).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# errr...\n",
    "\n",
    "But, hey, our R-squared keeps going up ... we must be doing something right!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# automating the data dredge\n",
    "\n",
    "Sometimes -- not when trying to make any claims about causation -- but sometimes when doing exploratory analyses, you might just want to automate your data dredging workflow.  Many statistical packages actually have something called \"stepwise regression\" that does this for the researcher.  Stepwise regression is meant to be exploratory, but because almost no one who publishes results pre-registers their experiments it has become a mainstay of the academic publishing dystopia.  People get data, run a bazillion transformations and interactions, then run stepwise regression to maximize their R-squared, increase their confidence intervals, and come out with a totally confabulated hypothesis that their analysis \"tests\".  \n",
    "\n",
    "In future weeks, we'll start to see how this kind of automation, when done correctly, can be a good thing.  But for now, let's just see how we would do this in Python, which does not have a package to do this.  We'll just make a simple function that can iterate through a fully interacted model.  \n",
    "\n",
    "You don't have to understand all the code to know this: many researchers do things like this all the time, building up extremely fragile models that increase the fit fo the their model, but have no theory and are never really thought through.  Needless to say, you shouldn't do it.  But with that advice, let's do something really stupid, so we can learn a bit about how sci-kit learn work later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_mining(data, dv):\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(dv)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(dv, ' * '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(dv, ' * '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvars = ['crime_log', 'income_log', 'population_density', 'FAGI_TOTAL_2010']\n",
    "\n",
    "df = pd.DataFrame(tracts_crimes[myvars])\n",
    "mining = data_mining(df,'crime_log')\n",
    "mining.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding more variables\n",
    "Ok, now add the following to our list of variables to myvars: \n",
    "```\n",
    "'percent_asian', 'percent_white', 'percent_black'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvars = ['crime_log', 'income_log', 'population_density', 'FAGI_TOTAL_2010', 'percent_asian', 'percent_white', 'percent_black']\n",
    "\n",
    "df = pd.DataFrame(tracts_crimes[myvars])\n",
    "mining = data_mining(df,'crime_log')\n",
    "mining.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, the R-squared went up again!  We must be geniuses.  \n",
    "\n",
    "OK, enough of the bad stats practices. We'll soon see how to turn these to the power of good.  But please just remember: with great power..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
